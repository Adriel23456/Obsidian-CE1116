---
Fecha de creaci√≥n: 2025-08-29 19:10
Fecha de Modificaci√≥n: 2025-08-29 19:10
tags: [tokenization, bpe, wordpiece, nlp-preprocessing, subword-segmentation]
Tema: Inteligencia Artificial
---

## üìö Idea/Concepto 
- La **tokenizaci√≥n** transforma texto en secuencias de enteros v√≠a segmentaci√≥n learned o rule-based. Algoritmos principales: BPE aprende fusiones greedy de pares frecuentes; WordPiece optimiza log-likelihood unigram; Unigram Language Model poda vocabulario v√≠a probabilidades marginales. GPT usa byte-level BPE sobre bytes UTF-8 (vocab ~50k), eliminando tokens UNK pero aumentando longitud para texto no-ASCII. El pipeline: texto ‚Üí normalizaci√≥n (NFKC, casing) ‚Üí pre-tokenizaci√≥n (split whitespace/punctuation) ‚Üí subword splitting ‚Üí mapeo a IDs. Tokens especiales estructuran input/output: padding [PAD], unknown [UNK], mask [MASK], separadores ([CLS], [SEP], <|endoftext|>), y control tokens para fine-tuning. Trade-offs cr√≠ticos: vocabulario grande (mejor coverage, m√°s par√°metros en embeddings) vs peque√±o (secuencias m√°s largas, mejor generalizaci√≥n); granularidad character-level (flexible, secuencias largas) vs word-level (sem√°nticamente rico, ineficiente para OOV). Fertility (tokens/palabra) afecta directamente uso del contexto: idiomas no-latinos pueden tener 3-10x fertility vs ingl√©s, reduciendo contexto efectivo. Tokenizer training aprende merges/vocabulario en corpus; inference debe usar modelo id√©ntico o rompe embeddings. Reversibilidad requiere preservar espacios/casing en vocabulario.

## üìå Puntos Claves
- **BPE (Byte Pair Encoding)**: Fusiona iterativamente pares de caracteres frecuentes
- **Vocabulario t√≠pico**: 30k-50k tokens (GPT-3: 50,257, Llama 2: 32,000)
- **Tokens especiales**: [CLS], [SEP], [PAD], [MASK], <|endoftext|>
- **Fertility problem**: Idiomas no-latinos requieren m√°s tokens, reduciendo contexto efectivo
- **Determinismo**: Tokenizer debe ser id√©ntico en training e inference
- **Trade-offs**: Balance entre tama√±o vocabulario y longitud de secuencias

## üîó Connections
- [[Embeddings]] - Los token IDs producidos indexan la matriz de embeddings
- [[Ventana de Contexto]] - Fertility afecta directamente el contexto efectivo disponible
- [[Large Language Models (LLMs)]] - Primer paso del pipeline de procesamiento en LLMs
- [[Redes Neuronales]] - Convierte texto en formato num√©rico procesable
- [[Mecanismo de atenci√≥n]] - Define las unidades at√≥micas sobre las que opera attention

## üí° Personal Insight
- La tokenizaci√≥n es el h√©roe no reconocido del NLP moderno. Un mal tokenizer puede arruinar incluso el mejor modelo. El problema de fertility es especialmente frustrante: modelos entrenados principalmente en ingl√©s penalizan sistem√°ticamente otros idiomas al darles menos contexto efectivo. Es un sesgo arquitectural profundo que necesitamos resolver.

## üßæ Recursos
- Karpathy, A. - "[1hr Talk] Intro to Large Language Models"
- Madriz H., D. - "CE 1116 Dise√±o y Calidad de Productos Tecnol√≥gicos"