---
Fecha de creaciÃ³n: 2025-08-29 18:10
Fecha de ModificaciÃ³n: 2025-08-29 18:10
tags: [attention-mechanism, transformers, self-attention, multi-head, deep-learning]
Tema: Inteligencia Artificial
---

## ğŸ“š Idea/Concepto 
- El **mecanismo de atenciÃ³n** en Transformers mapea queries a una suma ponderada de values mediante compatibilidad con keys. MatemÃ¡ticamente: Attention(Q,K,V) = softmax(QK^T/âˆšdk)V, donde dk=dmodel/h para multi-head. Cada cabeza opera en subespacios diferentes: headi = Attention(QWQi, KWKi, VWVi), luego se concatenan y proyectan: MultiHead(Q,K,V) = Concat(head1...headh)WO. Self-attention (Q=K=V=X) permite modelar dependencias intra-secuencia; masked self-attention previene atenciÃ³n a posiciones futuras mediante mÃ¡scara -âˆ; cross-attention usa Q del decoder con K,V del encoder para alineaciÃ³n. Computacionalmente es O(nÂ²d) en memoria pero O(1) en profundidad vs O(n) de RNNs. Los patrones de atenciÃ³n emergen durante entrenamiento: cabezas superficiales capturan relaciones locales/sintÃ¡cticas, profundas capturan semÃ¡ntica/dependencias largas. Positional encodings se suman a embeddings porque la atenciÃ³n es permutation-invariant. Limitaciones incluyen complejidad cuadrÃ¡tica en longitud de secuencia y dificultad para generalizar a secuencias mÃ¡s largas que las vistas en entrenamiento.

## ğŸ“Œ Puntos Claves
- **Query, Key, Value**: Proyecciones aprendidas que determinan relevancia entre tokens
- **Scaled dot-product**: QK^T/âˆšdk previene saturaciÃ³n del softmax
- **Multi-head attention**: MÃºltiples cabezas capturan diferentes tipos de relaciones
- **Self vs Cross-attention**: Intra-secuencia vs inter-secuencia (encoder-decoder)
- **Complejidad O(nÂ²)**: CuadrÃ¡tica en longitud, limita secuencias largas
- **Permutation-invariant**: Requiere positional encodings para orden

## ğŸ”— Connections
- [[Large Language Models (LLMs)]] - Componente fundamental de arquitecturas transformer
- [[Redes Neuronales]] - Mecanismo que reemplaza recurrencia en RNNs
- [[Ventana de Contexto]] - Complejidad cuadrÃ¡tica determina lÃ­mites prÃ¡cticos
- [[Embeddings]] - Entrada que se transforma en Q, K, V mediante proyecciones
- [[TokenizaciÃ³n]] - Define las unidades sobre las que opera la atenciÃ³n

## ğŸ’¡ Personal Insight
- La atenciÃ³n es "todo lo que necesitas" porque permite que cada token vea toda la secuencia simultÃ¡neamente, eliminando el cuello de botella secuencial de RNNs. La genialidad estÃ¡ en que es diferenciable y paralelizable. Sin embargo, la complejidad cuadrÃ¡tica sigue siendo el talÃ³n de Aquiles para secuencias muy largas.

## ğŸ§¾ Recursos
- Vaswani et al. (2017) - "Attention Is All You Need"
- 3Blue1Brown - "Attention in transformers, step-by-step | Deep Learning Chapter 6"
- 3Blue1Brown - "Transformers, the tech behind LLMs | Deep Learning Chapter 5"