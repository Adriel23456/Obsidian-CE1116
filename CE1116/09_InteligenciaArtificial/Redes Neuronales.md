---
Fecha de creaci√≥n: 2025-08-29 17:30
Fecha de Modificaci√≥n: 2025-08-29 17:30
tags: [redes-neuronales, deep-learning, ai, machine-learning, neural-networks]
Tema: Inteligencia Artificial
---

## üìö Idea/Concepto 
- Las **Redes Neuronales** son una familia de modelos matem√°ticos y sistemas computacionales inspirados en el cerebro que aprenden a mapear entradas a salidas. Est√°n armadas en capas con 'neuronas' conectadas por pesos y sesgos, los cuales son los par√°metros que el entrenamiento ajusta. La clave para capturar relaciones complejas son las funciones de activaci√≥n, sin ellas, todo ser√≠a lineal. Aprenden con el ciclo forward - p√©rdida - gradientes - actualizaci√≥n (descenso de gradiente + retropropagaci√≥n que reparte el error hacia atr√°s). Ya entrenadas y en producci√≥n, solo se ejecuta el forward: no se tocan los par√°metros. Si es clasificaci√≥n, los logits pasan por softmax para leer probabilidades y elegir la clase m√°s probable. M√°s profundidad y conexiones suman capacidad, pero lo que importa es c√≥mo quedan ajustados los par√°metros, qu√© activaciones usan y c√≥mo se entrenan, apuntando a generalizar bien en datos no vistos (evitando sobreajuste y subajuste). Y adem√°s, hay arquitecturas seg√∫n el tipo de dato: CNN para im√°genes, RNN/LSTM/GRU para secuencias y Transformers para texto y tareas de lenguaje.

## üìå Puntos Claves
- **Arquitectura b√°sica**: Capas de neuronas conectadas por pesos y sesgos ajustables
- **Funciones de activaci√≥n**: ReLU, Sigmoid, tanh - introducen no-linealidad esencial
- **Proceso de aprendizaje**: Forward pass ‚Üí c√°lculo de p√©rdida ‚Üí backpropagation ‚Üí actualizaci√≥n de pesos
- **Modo inferencia**: Solo forward pass, par√°metros congelados
- **Arquitecturas especializadas**: CNN (im√°genes), RNN/LSTM (secuencias), Transformers (texto)
- **Generalizaci√≥n**: Balance entre underfitting y overfitting mediante regularizaci√≥n

## üîó Connections
- [[Large Language Models (LLMs)]] - Los LLMs son redes neuronales masivas basadas en transformers
- [[Mecanismo de atenci√≥n]] - Componente clave en arquitecturas transformer modernas
- [[Embeddings]] - Capa inicial que convierte tokens en vectores para procesamiento neuronal
- [[Tokenizaci√≥n]] - Preprocesamiento necesario antes de alimentar texto a la red
- [[Ventana de Contexto]] - Limitaci√≥n arquitectural en redes transformer

## üí° Personal Insight
- Las redes neuronales no "entienden" realmente, sino que aprenden patrones estad√≠sticos complejos. El verdadero poder viene de la composici√≥n de transformaciones simples en capas profundas. Lo m√°s importante no es la cantidad de neuronas, sino c√≥mo se entrenan y regularizan para generalizar bien.

## üßæ Recursos
- 3Blue1Brown - "But what is a neural network? | Deep learning chapter 1"
- Nielsen, M. - "Neural networks and deep learning"
- Madriz H., D. - "CE 1116 Dise√±o y Calidad de Productos Tecnol√≥gicos"