---
Fecha de creaci√≥n: 2025-08-29 18:30
Fecha de Modificaci√≥n: 2025-08-29 18:30
tags: [context-window, transformers, memory-limitations, attention-complexity, llm-constraints]
Tema: Inteligencia Artificial
---

## üìö Idea/Concepto 
- La **ventana de contexto** es el l√≠mite de tokens que un modelo puede procesar simult√°neamente, determinado por la complejidad O(n¬≤¬∑d) de self-attention donde n=longitud, d=dimensi√≥n. Requiere almacenar matrices de atenci√≥n n√ón√ónum_heads√ónum_layers, escalando cuadr√°ticamente en memoria (ej: 100k tokens ‚âà 40GB solo para scores). Los modelos se entrenan con positional encodings hasta longitud m√°xima L; extrapolar m√°s all√° degrada performance ("length generalization problem"). El effective context length es menor que el te√≥rico: estudios muestran degradaci√≥n en retrieval accuracy con distancia ("lost in the middle"). Estrategias de manejo incluyen: truncamiento (sliding window, mantener inicio/fin), compresi√≥n (resumir chunks antiguos), o memoria externa (retrieval-augmented generation). Optimizaciones como Flash Attention reducen memoria v√≠a tiling pero mantienen O(n¬≤); alternativas como Linformer/Performer aproximan con O(n). El KV-cache en inferencia almacena keys/values pasados, requiriendo memoria proporcional a batch_size√óseq_len√óhidden_dim. Ventanas actuales: 4k-8k (modelos est√°ndar), 32k-128k (optimizados), 1M+ (modelos especializados como Gemini 1.5).

## üìå Puntos Claves
- **Complejidad O(n¬≤)**: Memoria escala cuadr√°ticamente con longitud de secuencia
- **L√≠mite hardware**: GPUs tienen memoria finita para almacenar attention scores
- **Effective vs theoretical**: Performance degrada antes del l√≠mite te√≥rico
- **"Lost in the middle"**: Informaci√≥n en medio de contextos largos se pierde
- **KV-cache**: Optimizaci√≥n para inferencia que consume memoria adicional
- **Trade-offs**: Ventana grande = m√°s contexto pero m√°s costo computacional

## üîó Connections
- [[Mecanismo de atenci√≥n]] - La complejidad cuadr√°tica de attention define los l√≠mites
- [[Large Language Models (LLMs)]] - Caracter√≠stica limitante fundamental de LLMs
- [[Tokenizaci√≥n]] - Fertility afecta contexto efectivo (m√°s tokens por palabra = menos contexto)
- [[Embeddings]] - Positional encodings limitan extrapolaci√≥n a secuencias largas
- [[Redes Neuronales]] - Restricci√≥n arquitectural de transformers vs RNNs

## üí° Personal Insight
- La ventana de contexto es quiz√°s la limitaci√≥n m√°s frustrante de los LLMs actuales. No es solo un problema de memoria, sino de c√≥mo el modelo realmente usa ese contexto. Incluso con ventanas de 100k tokens, los modelos tienden a "olvidar" informaci√≥n del medio. RAG (retrieval-augmented generation) es una soluci√≥n pragm√°tica hasta que tengamos attention sub-cuadr√°tica efectiva.

## üßæ Recursos
- IBM Technology - "Transforming Language with Generative Pre-trained Transformers (GPT)"
- Madriz H., D. - "CE 1116 Dise√±o y Calidad de Productos Tecnol√≥gicos"