---
Fecha de creaci√≥n: 2025-08-29 18:50
Fecha de Modificaci√≥n: 2025-08-29 18:50
tags: [embeddings, word-embeddings, vector-representations, nlp, transformers]
Tema: Inteligencia Artificial
---

## üìö Idea/Concepto 
- Los **embeddings** son proyecciones aprendibles de tokens discretos a vectores continuos densos. La matriz de embeddings E ‚àà R^(|V|√ód) mapea vocabulario V a dimensi√≥n d v√≠a lookup: ei = E[token_id]. En transformers, el embedding final combina m√∫ltiples componentes: token embeddings (sem√°ntica), positional embeddings (orden v√≠a sinusoides o learned), y opcionalmente segment/type embeddings (BERT). Matem√°ticamente: input = TokenEmb(x) + PosEmb(pos) + TypeEmb(type). Weight tying (compartir E entre input/output) reduce par√°metros y mejora generalizaci√≥n. Los embeddings aprenden estructura del lenguaje: relaciones geom√©tricas codifican analog√≠as sem√°nticas/sint√°cticas (medibles v√≠a cosine similarity), clustering captura categor√≠as conceptuales, y las dimensiones individuales pueden correlacionar con propiedades ling√º√≠sticas espec√≠ficas. Subword tokenization (BPE/WordPiece) permite embeddings compositivos: palabras raras se construyen desde subunidades frecuentes. Durante training, embeddings se inicializan ~N(0, 0.02) y se optimizan v√≠a gradientes como cualquier par√°metro. Los embeddings contextualizados de capas superiores (vs est√°ticos de entrada) capturan significado dependiente del contexto, resolviendo polisemia.

## üìå Puntos Claves
- **Lookup table**: Matriz E donde cada fila es un vector denso para cada token
- **Alta dimensionalidad**: T√≠picamente 128-4096 dimensiones (GPT-3: 12,288)
- **Componentes combinados**: Token + positional + type embeddings
- **Propiedades algebraicas**: vec(king) - vec(man) + vec(woman) ‚âà vec(queen)
- **Weight tying**: Compartir embeddings entrada/salida reduce par√°metros
- **Contextualizaci√≥n**: Self-attention transforma embeddings est√°ticos en contextuales

## üîó Connections
- [[Tokenizaci√≥n]] - Define los IDs que indexan la matriz de embeddings
- [[Mecanismo de atenci√≥n]] - Transforma embeddings en Q, K, V para procesamiento
- [[Redes Neuronales]] - Primera capa que convierte s√≠mbolos discretos en vectores continuos
- [[Large Language Models (LLMs)]] - Componente fundamental de la arquitectura transformer
- [[Ventana de Contexto]] - Positional embeddings limitan generalizaci√≥n a secuencias largas

## üí° Personal Insight
- Los embeddings son quiz√°s el concepto m√°s elegante en NLP moderno: convierten el problema simb√≥lico discreto del lenguaje en uno geom√©trico continuo. La magia est√° en que durante el entrenamiento, palabras con roles similares naturalmente se agrupan en el espacio vectorial, creando una "geograf√≠a del significado" navegable matem√°ticamente.

## üßæ Recursos
- IBM Technology - "What are Word Embeddings?"
- Madriz H., D. - "CE 1116 Dise√±o y Calidad de Productos Tecnol√≥gicos"
- 3Blue1Brown - "Transformers, the tech behind LLMs | Deep Learning Chapter 5"